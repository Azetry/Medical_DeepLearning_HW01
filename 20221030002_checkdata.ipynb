{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- [transfer-1](https://officeguide.cc/pytorch-transfer-learning-resnet18-classify-mnist-tutorial-examples/)\n",
    "- [transfer-2](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
    "- [train, val subset](https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets/50544887#50544887)\n",
    "\n",
    "---\n",
    "- [pytorch训练模型时出现nan原因整合](https://blog.csdn.net/ytusdc/article/details/122321907)\n",
    "```\n",
    "原因: 输入中就含有NaN 或者 trainingsample中出现了脏数据！脏数据的出现导致logits计算出了0，0传给  即nan。\n",
    "\n",
    "现象:每当学习的过程中碰到这个错误的输入，就会变成NaN。观察loss的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了\n",
    "解决方法：逐步去定位错误数据,然后删掉这部分数据.\n",
    "通过设置batch_size = 1，shuffle = False，一步一步地将sample定位到了所有可能的脏数据，删掉\n",
    "重整数据集，确保训练集和验证集里面没有损坏的图片。可以使用一个简单的网络去读取输入,如果有一个数据是错误的,这个网络的loss值也会出现Nan\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pydicom\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, io\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "from package.dataset import DicomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(tr: torch.Tensor):\n",
    "    infos = {\n",
    "        'min': torch.amin(tr),\n",
    "        'max': torch.amax(tr),\n",
    "        'dtype': tr.dtype,\n",
    "        'size': tr.size()\n",
    "    }\n",
    "\n",
    "    return infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "validation_split = .2\n",
    "shuffle_dataset = False\n",
    "random_seed= 2022\n",
    "num_epochs = 10\n",
    "conv_threshold = 30\n",
    "\n",
    "lr = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>FilePath</th>\n",
       "      <th>index</th>\n",
       "      <th>Stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A175204</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>/DICOM/A175204/00010018</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A122221</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>/DICOM/A122221/00010034</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A54671</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>/DICOM/A54671/00010021</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A31117</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>/DICOM/A31117/00010022</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A653195</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>/DICOM/A653195/00010016</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>A717094</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>/DICOM/A717094/00010022</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>A741758</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>/DICOM/A741758/00010017</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>A646753</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "      <td>/DICOM/A646753/00010023</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>A679904</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>/DICOM/A679904/00010020</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>A156039</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>/DICOM/A156039/00010036</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>161 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  Age  Gender                 FilePath  index  Stage\n",
       "0    A175204   69       0  /DICOM/A175204/00010018      7      1\n",
       "1    A122221   56       0  /DICOM/A122221/00010034     15      1\n",
       "2     A54671   82       0   /DICOM/A54671/00010021      8      1\n",
       "3     A31117   71       1   /DICOM/A31117/00010022     10      1\n",
       "4    A653195   68       0  /DICOM/A653195/00010016      7      1\n",
       "..       ...  ...     ...                      ...    ...    ...\n",
       "156  A717094   80       1  /DICOM/A717094/00010022     11      3\n",
       "157  A741758   54       1  /DICOM/A741758/00010017      9      3\n",
       "158  A646753   75       1  /DICOM/A646753/00010023     12      3\n",
       "159  A679904   62       1  /DICOM/A679904/00010020      9      3\n",
       "160  A156039   62       1  /DICOM/A156039/00010036     17      3\n",
       "\n",
       "[161 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"./data/DICOM/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.CenterCrop(50), transforms.Resize(224),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DicomDataset(root=\"./data\", transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('Stage 1', 'Stage 2', 'Stage 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'min': tensor(0.),\n",
       " 'max': tensor(258.3909),\n",
       " 'dtype': torch.float32,\n",
       " 'size': torch.Size([3, 224, 224])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "display(dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Length: 129\n",
      "Validation Data Length: 32\n"
     ]
    }
   ],
   "source": [
    "#  Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "print(f\"Train Data Length: { len(train_indices) }\")\n",
    "print(f\"Validation Data Length: { len(val_indices) }\")\n",
    "\n",
    "dataset_sizes = {\n",
    "    'train': len(train_indices),\n",
    "    'val': len(val_indices),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(dataset, batch_size=batch_size, sampler=train_sampler),\n",
    "    'val': DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/azetry/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/azetry/.conda/envs/torch/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/azetry/.conda/envs/torch/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16_bn', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): ReLU(inplace=True)\n",
      "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): ReLU(inplace=True)\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU(inplace=True)\n",
      "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (36): ReLU(inplace=True)\n",
       "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (39): ReLU(inplace=True)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier[6] = nn.Linear(in_features=4096, out_features=3, bias=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = training_data[0][0].unsqueeze(0)\n",
    "# with torch.no_grad():\n",
    "#     output = model(test)\n",
    "\n",
    "# print(output[0])\n",
    "# print( torch.nn.functional.softmax(output[0], dim=0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloader, model, loss_fn, optimizer, num_epochs):\n",
    "    since = time.time()\n",
    "\n",
    "    # 儲存最佳參數\n",
    "    prev_acc = 0.0\n",
    "    best_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # 計算是否收斂和提前結束\n",
    "    count_cont = 0\n",
    "    finish = False\n",
    "\n",
    "    # Level: Epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}/{num_epochs-1}:\")\n",
    "        print(\"-\"*8)\n",
    "\n",
    "        # 每次 epoch 都要跑一次 training 和 validation\n",
    "        # Level: Phase (train, val)\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train': model.train()\n",
    "            else: model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # 批次讀取資料進行訓練\n",
    "            # Level: Batch Data\n",
    "            for batch, (X, y) in enumerate(dataloader[phase]):\n",
    "                # 將資料放置於 GPU 或 CPU\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "                # optimizer.zero_grad() # 重設參數梯度（gradient）\n",
    "\n",
    "                # forward\n",
    "                # 只有在訓練階段才要計算梯度\n",
    "                with torch.set_grad_enabled(phase == 'train'): # phase = True or False\n",
    "                    outputs = model(X)                  # 計算預測值\n",
    "                    print(outputs)\n",
    "                    _, preds = torch.max(outputs, 1)    # 計算預測結果\n",
    "                    loss = loss_fn(outputs, y)          # 計算損失值（loss）\n",
    "\n",
    "                    # 只有在訓練階段才要優化\n",
    "                    if phase == 'train':\n",
    "                        optimizer.zero_grad()           # 重設參數梯度（gradient）\n",
    "                        loss.backward()                 # 反向傳播（backpropagation）\n",
    "                        optimizer.step()                # 更新參數\n",
    "\n",
    "                # 統計\n",
    "                running_loss += loss.item() * Ｘ.size(0) # Batch size\n",
    "                running_corrects += torch.sum(preds == y.data)\n",
    "            # End of Level: Batch Data\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            if phase == 'train':\n",
    "                if epoch_acc == prev_acc: count_cont += 1\n",
    "                else: count_cont = 0\n",
    "                prev_acc = epoch_acc\n",
    "\n",
    "                if count_cont > conv_threshold: \n",
    "                    print(\"Convergence. Ｅnd training early.\")\n",
    "                    finish = True\n",
    "                    break\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        # End of Level: Phase (train, val)\n",
    "\n",
    "        print(\"-\"*8)\n",
    "        if finish: break\n",
    "    # End of Level: Epoch\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # 載入模型最佳參數\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9:\n",
      "--------\n",
      "tensor([[-0.5438,  0.0574,  0.5229]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ -61.3873, -110.7859,  170.6068]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-198.2621, -362.8061,  561.0560]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ -190.6390,  7385.9478, -7193.1895]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.2086e+09, -3.2401e+09,  3.1160e+07]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-8.9188e+18,  9.0304e+18, -1.1164e+17]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[4.5881e+33,       -inf,        inf]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "train Loss: nan Acc: 0.1085\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "val Loss: nan Acc: 1.0000\n",
      "--------\n",
      "Epoch 1/9:\n",
      "--------\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m train_model(dataloaders, model, criterion, optimizer, num_epochs)\n",
      "Cell \u001b[0;32mIn [17], line 38\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(dataloader, model, loss_fn, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39m# optimizer.zero_grad() # 重設參數梯度（gradient）\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[39m# forward\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m# 只有在訓練階段才要計算梯度\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m): \u001b[39m# phase = True or False\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     outputs \u001b[39m=\u001b[39m model(X)                  \u001b[39m# 計算預測值\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[39mprint\u001b[39m(outputs)\n\u001b[1;32m     40\u001b[0m     _, preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs, \u001b[39m1\u001b[39m)    \u001b[39m# 計算預測結果\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torchvision/models/vgg.py:66\u001b[0m, in \u001b[0;36mVGG.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 66\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[1;32m     67\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[1;32m     68\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_model(dataloaders, model, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更改超參數 (ex: lr) 輸出出現 NaN 的比數不同，應不是資料本身數值問題\n",
    "- lr=1e4: no. 3\n",
    "- lr=0.2: no. 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"20221030002_checkdata.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch(azetry)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
