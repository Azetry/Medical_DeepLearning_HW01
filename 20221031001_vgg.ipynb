{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pydicom\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, io\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f82847c2710>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "np.random.seed(2022)\n",
    "random.seed(2022)\n",
    "torch.manual_seed(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(tr: torch.Tensor):\n",
    "    infos = {\n",
    "        'min': torch.amin(tr),\n",
    "        'max': torch.amax(tr),\n",
    "        'dtype': tr.dtype,\n",
    "        'size': tr.size()\n",
    "    }\n",
    "\n",
    "    return infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "split = .8\n",
    "shuffle_dataset = True\n",
    "random_seed= 2022\n",
    "num_epochs = 10\n",
    "conv_threshold = 30\n",
    "\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPECTDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    - split data into train, val (frac, 1-frac)\n",
    "    - random_state set 2022 (fix random result)\n",
    "    '''\n",
    "    def __init__(self, root, train, frac, transform):\n",
    "        self.root = Path(root)\n",
    "        self.transform = transform\n",
    "        df = pd.read_csv( str(self.root/ \"DICOM/train.csv\") )\n",
    "\n",
    "        # Train / Validation data\n",
    "        train_df = df.sample(frac=frac, random_state=2022, ignore_index=True)\n",
    "        if train: self.list = train_df\n",
    "        else: self.list = pd.concat( [df, train_df] ).drop_duplicates(keep=False, ignore_index=True)\n",
    "\n",
    "        # edit file path\n",
    "        self.list.FilePath = self.list.FilePath.apply(lambda _: self.root / _[1:])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dcm = pydicom.read_file( str(self.list.FilePath[idx]) )\n",
    "\n",
    "        # age, gender\n",
    "        age = self.list.loc[idx, 'Age']\n",
    "        gender = self.list.loc[idx, 'Gender']\n",
    "        \n",
    "        # label (1,2,3 -> 0.,1,2)\n",
    "        label = int(self.list.Stage[idx]) - 1\n",
    "\n",
    "        # Preprocessed Pixels: totensor, 3 channel\n",
    "        pixel = dcm.pixel_array[ self.list.loc[idx, 'index'] ] # 用 index 當 column name 真的是天才\n",
    "        # low, high = self.get_low_high(dcm)\n",
    "        # pixeled = self.getWindow(pixel, low, high)\n",
    "        # img = (pixeled - np.min(pixeled)) / (np.max(pixeled) - np.min(pixeled))\n",
    "        img = torch.tensor(pixel.astype(np.float32))\n",
    "        img = torch.stack([img, img, img], dim=0)\n",
    "\n",
    "        seed = np.random.randint(1e9)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        img = self.transform(img)\n",
    "\n",
    "        return img, age, gender, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.CenterCrop(50), \n",
    "    # transforms.Normalize((62.2852, 62.2852, 62.2852), (76.8448, 76.8448, 76.8448)), # 跑 normalize 反而下降準確度\n",
    "    transforms.Resize(224),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = SPECTDataset(root=\"./data\", train=True, frac=split, transform=preprocess)\n",
    "validation_data = SPECTDataset(root=\"./data\", train=False, frac=split, transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練資料集數量： 129\n",
      "測試資料集數量： 32\n"
     ]
    }
   ],
   "source": [
    "print(\"訓練資料集數量：\", len(training_data))\n",
    "print(\"測試資料集數量：\", len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train': DataLoader(training_data, batch_size=batch_size, shuffle=shuffle_dataset),\n",
    "    'val': DataLoader(validation_data, batch_size=batch_size, shuffle=shuffle_dataset)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([16, 3, 224, 224])\n",
      "Shape of y:  torch.Size([16]) torch.int64\n",
      "Age:  tensor([25, 64, 74, 48, 81, 75, 54, 77, 75, 64, 50, 80, 67, 85, 72, 66])\n",
      "Gender:  tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for X, age, gender, y in dataloaders['val']:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    print(\"Age: \", age)\n",
    "    print(\"Gender: \", gender)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/azetry/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/azetry/.conda/envs/torch/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/azetry/.conda/envs/torch/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPECT_VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SPECT_VGG16, self).__init__()\n",
    "\n",
    "        # 載入 VGG16 類神經網路結構\n",
    "        self.model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)\n",
    "\n",
    "        # 鎖定 VGG16 預訓練模型參數\n",
    "        for param in self.model.parameters():\n",
    "           param.requires_grad = False\n",
    "\n",
    "        # 修改輸出層輸出數量\n",
    "        self.model.classifier.add_module(\"7\", nn.Linear(in_features=1000, out_features=20))\n",
    "\n",
    "    def forward(self, x, age, gender):\n",
    "        logits_ = self.model(x)\n",
    "\n",
    "        # Add Age and Gender\n",
    "        age.unsqueeze_(1)\n",
    "        logits_ = torch.cat((logits_, age), dim=1)\n",
    "\n",
    "        gender.unsqueeze_(1)\n",
    "        logits_ = torch.cat((logits_, gender), dim=1)\n",
    "\n",
    "        # Final Classifier (這樣寫不太好，但就先可以run)\n",
    "        logits = nn.Linear(22, 3).to(device)(logits_)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/azetry/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPECT_VGG16(\n",
      "  (model): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (15): ReLU(inplace=True)\n",
      "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (18): ReLU(inplace=True)\n",
      "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (20): ReLU(inplace=True)\n",
      "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (22): ReLU(inplace=True)\n",
      "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (25): ReLU(inplace=True)\n",
      "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (27): ReLU(inplace=True)\n",
      "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (29): ReLU(inplace=True)\n",
      "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "      (7): Linear(in_features=1000, out_features=20, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SPECT_VGG16().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloader, model, loss_fn, optimizer, num_epochs):\n",
    "    since = time.time()\n",
    "\n",
    "    # 儲存最佳參數\n",
    "    prev_acc = 0.0\n",
    "    best_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # 計算是否收斂和提前結束\n",
    "    count_cont = 0\n",
    "    finish = False\n",
    "\n",
    "    # Level: Epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}/{num_epochs-1}:\")\n",
    "        print(\"-\"*8)\n",
    "\n",
    "        # 每次 epoch 都要跑一次 training 和 validation\n",
    "        # Level: Phase (train, val)\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train': model.train()\n",
    "            else: model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # 批次讀取資料進行訓練\n",
    "            # Level: Batch Data\n",
    "            for batch, (X, age, gender, y) in enumerate(dataloader[phase]):\n",
    "                # 將資料放置於 GPU 或 CPU\n",
    "                X, age, gender, y = X.to(device), age.to(device), gender.to(device), y.to(device)\n",
    "\n",
    "                optimizer.zero_grad() # 重設參數梯度（gradient）\n",
    "\n",
    "                # forward\n",
    "                # 只有在訓練階段才要計算梯度\n",
    "                with torch.set_grad_enabled(phase == 'train'): # phase = True or False\n",
    "                    outputs = model(X, age, gender)                  # 計算預測值\n",
    "                    _, preds = torch.max(outputs, 1)    # 計算預測結果\n",
    "                    loss = loss_fn(outputs, y)          # 計算損失值（loss）\n",
    "\n",
    "                    # 只有在訓練階段才要優化\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()                 # 反向傳播（backpropagation）\n",
    "                        optimizer.step()                # 更新參數\n",
    "\n",
    "                # 統計\n",
    "                running_loss += loss.item() * X.size(0) # Batch size\n",
    "                running_corrects += torch.sum(preds == y.data)\n",
    "            # End of Level: Batch Data\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloader[phase].dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            if phase == 'train':\n",
    "                if epoch_acc == prev_acc: count_cont += 1\n",
    "                else: count_cont = 0\n",
    "                prev_acc = epoch_acc\n",
    "\n",
    "                if count_cont > conv_threshold: \n",
    "                    print(\"Convergence. End training early.\")\n",
    "                    finish = True\n",
    "                    break\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        # End of Level: Phase (train, val)\n",
    "\n",
    "        print(\"-\"*8)\n",
    "        if finish: break\n",
    "    # End of Level: Epoch\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # 載入模型最佳參數\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9:\n",
      "--------\n",
      "train Loss: 15.4169 Acc: 0.2868\n",
      "val Loss: 8.9965 Acc: 0.5000\n",
      "--------\n",
      "Epoch 1/9:\n",
      "--------\n",
      "train Loss: 28.0438 Acc: 0.3333\n",
      "val Loss: 21.4161 Acc: 0.2812\n",
      "--------\n",
      "Epoch 2/9:\n",
      "--------\n",
      "train Loss: 28.1416 Acc: 0.3411\n",
      "val Loss: 34.0151 Acc: 0.3438\n",
      "--------\n",
      "Epoch 3/9:\n",
      "--------\n",
      "train Loss: 21.0863 Acc: 0.3488\n",
      "val Loss: 23.2798 Acc: 0.2500\n",
      "--------\n",
      "Epoch 4/9:\n",
      "--------\n",
      "train Loss: 18.4104 Acc: 0.3411\n",
      "val Loss: 39.2855 Acc: 0.2812\n",
      "--------\n",
      "Epoch 5/9:\n",
      "--------\n",
      "train Loss: 33.2450 Acc: 0.3023\n",
      "val Loss: 44.3625 Acc: 0.3750\n",
      "--------\n",
      "Epoch 6/9:\n",
      "--------\n",
      "train Loss: 34.3497 Acc: 0.3256\n",
      "val Loss: 25.0042 Acc: 0.3125\n",
      "--------\n",
      "Epoch 7/9:\n",
      "--------\n",
      "train Loss: 42.8089 Acc: 0.3876\n",
      "val Loss: 18.7734 Acc: 0.3125\n",
      "--------\n",
      "Epoch 8/9:\n",
      "--------\n",
      "train Loss: 31.5940 Acc: 0.3256\n",
      "val Loss: 19.8369 Acc: 0.2812\n",
      "--------\n",
      "Epoch 9/9:\n",
      "--------\n",
      "train Loss: 21.6501 Acc: 0.3023\n",
      "val Loss: 62.9105 Acc: 0.1875\n",
      "--------\n",
      "Training complete in 0m 10s\n",
      "Best val Acc: 0.500000\n"
     ]
    }
   ],
   "source": [
    "model = train_model(dataloaders, model, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"20221031001_vgg.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fdcc50b8790cd79eba7c7760378c0f44ab97d60599e9e289704406bba2ea7b42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
