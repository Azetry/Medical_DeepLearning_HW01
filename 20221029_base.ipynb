{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- [transfer-1](https://officeguide.cc/pytorch-transfer-learning-resnet18-classify-mnist-tutorial-examples/)\n",
    "- [transfer-2](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
    "- [train, val subset](https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets/50544887#50544887)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pydicom\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, io\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "from package.dataset import DicomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(tr: torch.Tensor):\n",
    "    infos = {\n",
    "        'min': torch.amin(tr),\n",
    "        'max': torch.amax(tr),\n",
    "        'dtype': tr.dtype,\n",
    "        'size': tr.size()\n",
    "    }\n",
    "\n",
    "    return infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 2022\n",
    "num_epochs = 10\n",
    "conv_threshold = 30\n",
    "\n",
    "lr = 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>FilePath</th>\n",
       "      <th>index</th>\n",
       "      <th>Stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A175204</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>/DICOM/A175204/00010018</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A122221</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>/DICOM/A122221/00010034</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A54671</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>/DICOM/A54671/00010021</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A31117</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>/DICOM/A31117/00010022</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A653195</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>/DICOM/A653195/00010016</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>A717094</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>/DICOM/A717094/00010022</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>A741758</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>/DICOM/A741758/00010017</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>A646753</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "      <td>/DICOM/A646753/00010023</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>A679904</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>/DICOM/A679904/00010020</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>A156039</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>/DICOM/A156039/00010036</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>161 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  Age  Gender                 FilePath  index  Stage\n",
       "0    A175204   69       0  /DICOM/A175204/00010018      7      1\n",
       "1    A122221   56       0  /DICOM/A122221/00010034     15      1\n",
       "2     A54671   82       0   /DICOM/A54671/00010021      8      1\n",
       "3     A31117   71       1   /DICOM/A31117/00010022     10      1\n",
       "4    A653195   68       0  /DICOM/A653195/00010016      7      1\n",
       "..       ...  ...     ...                      ...    ...    ...\n",
       "156  A717094   80       1  /DICOM/A717094/00010022     11      3\n",
       "157  A741758   54       1  /DICOM/A741758/00010017      9      3\n",
       "158  A646753   75       1  /DICOM/A646753/00010023     12      3\n",
       "159  A679904   62       1  /DICOM/A679904/00010020      9      3\n",
       "160  A156039   62       1  /DICOM/A156039/00010036     17      3\n",
       "\n",
       "[161 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"./data/DICOM/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.CenterCrop(50), transforms.Resize(224),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DicomDataset(root=\"./data\", transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('Stage 1', 'Stage 2', 'Stage 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'min': tensor(0.),\n",
       " 'max': tensor(258.3909),\n",
       " 'dtype': torch.float32,\n",
       " 'size': torch.Size([3, 224, 224])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "display(dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Length: 129\n",
      "Validation Data Length: 32\n"
     ]
    }
   ],
   "source": [
    "#  Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "print(f\"Train Data Length: { len(train_indices) }\")\n",
    "print(f\"Validation Data Length: { len(val_indices) }\")\n",
    "\n",
    "dataset_sizes = {\n",
    "    'train': len(train_indices),\n",
    "    'val': len(val_indices),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(dataset, batch_size=batch_size, sampler=train_sampler),\n",
    "    'val': DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/azetry/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/azetry/.conda/envs/torch/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/azetry/.conda/envs/torch/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier[6] = nn.Linear(in_features=4096, out_features=3, bias=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = training_data[0][0].unsqueeze(0)\n",
    "# with torch.no_grad():\n",
    "#     output = model(test)\n",
    "\n",
    "# print(output[0])\n",
    "# print( torch.nn.functional.softmax(output[0], dim=0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloader, model, loss_fn, optimizer, num_epochs):\n",
    "    since = time.time()\n",
    "\n",
    "    # 儲存最佳參數\n",
    "    prev_acc = 0.0\n",
    "    best_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # 計算是否收斂和提前結束\n",
    "    count_cont = 0\n",
    "    finish = False\n",
    "\n",
    "    # Level: Epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}/{num_epochs-1}:\")\n",
    "        print(\"-\"*8)\n",
    "\n",
    "        # 每次 epoch 都要跑一次 training 和 validation\n",
    "        # Level: Phase (train, val)\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train': model.train()\n",
    "            else: model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # 批次讀取資料進行訓練\n",
    "            # Level: Batch Data\n",
    "            for batch, (X, y) in enumerate(dataloader[phase]):\n",
    "                # 將資料放置於 GPU 或 CPU\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "                # optimizer.zero_grad() # 重設參數梯度（gradient）\n",
    "\n",
    "                # forward\n",
    "                # 只有在訓練階段才要計算梯度\n",
    "                with torch.set_grad_enabled(phase == 'train'): # phase = True or False\n",
    "                    outputs = model(X)                  # 計算預測值\n",
    "                    print(outputs)\n",
    "                    _, preds = torch.max(outputs, 1)    # 計算預測結果\n",
    "                    loss = loss_fn(outputs, y)          # 計算損失值（loss）\n",
    "\n",
    "                    # 只有在訓練階段才要優化\n",
    "                    if phase == 'train':\n",
    "                        optimizer.zero_grad()           # 重設參數梯度（gradient）\n",
    "                        loss.backward()                 # 反向傳播（backpropagation）\n",
    "                        optimizer.step()                # 更新參數\n",
    "\n",
    "                # 統計\n",
    "                running_loss += loss.item() * Ｘ.size(0) # Batch size\n",
    "                running_corrects += torch.sum(preds == y.data)\n",
    "            # End of Level: Batch Data\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            if phase == 'train':\n",
    "                if epoch_acc == prev_acc: count_cont += 1\n",
    "                else: count_cont = 0\n",
    "                prev_acc = epoch_acc\n",
    "\n",
    "                if count_cont > conv_threshold: \n",
    "                    print(\"Convergence. Ｅnd training early.\")\n",
    "                    finish = True\n",
    "                    break\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        # End of Level: Phase (train, val)\n",
    "\n",
    "        print(\"-\"*8)\n",
    "        if finish: break\n",
    "    # End of Level: Epoch\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # 載入模型最佳參數\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9:\n",
      "--------\n",
      "tensor([[ 0.7592,  0.5035, -5.2675]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "train Loss: nan Acc: 0.2558\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "val Loss: nan Acc: 0.3438\n",
      "--------\n",
      "Epoch 1/9:\n",
      "--------\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "train Loss: nan Acc: 0.2558\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan]], device='cuda:0')\n",
      "val Loss: nan Acc: 0.3438\n",
      "--------\n",
      "Epoch 2/9:\n",
      "--------\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m train_model(dataloaders, model, criterion, optimizer, num_epochs)\n",
      "Cell \u001b[0;32mIn [17], line 50\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(dataloader, model, loss_fn, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     47\u001b[0m             optimizer\u001b[39m.\u001b[39mstep()                \u001b[39m# 更新參數\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[39m# 統計\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem() \u001b[39m*\u001b[39m Ｘ\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m# Batch size\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     running_corrects \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(preds \u001b[39m==\u001b[39m y\u001b[39m.\u001b[39mdata)\n\u001b[1;32m     52\u001b[0m \u001b[39m# End of Level: Batch Data\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_model(dataloaders, model, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一直輸出 NaN，可能原因:\n",
    "- 學習率太大？(lr=0, 可以正常輸出)\n",
    "- 資料量太少、模型太深\n",
    "\n",
    "----\n",
    "\n",
    "- 重複資料，增加 random augmentation\n",
    "- normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"20221029_base.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch(azetry)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
